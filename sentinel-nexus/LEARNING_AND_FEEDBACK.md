# Learning, user feedback, and report length

What the AI learns from jobs, why users can add feedback, how we use it, and the maximum report length we support (including how to get 1–20 page reports).

---

## 1. What the AI has learned from (all jobs)

| Source | What we learn | Where it goes |
|--------|----------------|----------------|
| **Every run (brain)** | What we claimed, submitted, approved; what worked or failed; decisions (e.g. “skipped vague bounties”) | `~/.openclaw/workspace/cron-results/agency-report.txt`, **agency-learnings.md** |
| **Human-front jobs we deliver** | One line per delivered job: title, duration, bounty id | **agency-learnings.md** (appended after run cycle) |
| **User feedback + revise** | When a user sends feedback and we generate a revised report, we can append a short learning so the brain sees it | **agency-learnings.md** (see below) |

The brain reads the **last part** of `agency-learnings.md` each run (see dashboard API) and is instructed in HEARTBEAT to follow those learnings. So over time the system does more of what works and less of what doesn’t.

**We do not yet** learn from the *content* of user feedback in the brain’s prompts (e.g. “users often ask for more sources”). We do use feedback to **produce a revised report** for that job and **append a one-line learning** to `agency-learnings.md` when a revision is saved (e.g. “User feedback applied and revised report generated for: …”) so the brain is aware that feedback was used.

---

## 2. Why users can add feedback — and how we implement it

- **Why:** So we can **review the job result** and ask for changes. The user gets a revised report that addresses their feedback instead of a one-shot only.
- **Flow:**
  1. User opens **Track** → then **View full report** (`/job/report?token=...`).
  2. On the report page they can **Send feedback** (what to add or change). Feedback is stored in `workspace/job-feedback.json` by tracking token.
  3. They click **Revise report from feedback**. We call Kimi with: original deliverable + customer feedback → **updated report**.
  4. The **revised report** is stored in `workspace/revised-reports.json` and shown on the same page. So we **implement feedback and reproduce result** on our side (we don’t re-submit to ClawTasks; the user sees the revised version on the report page).
- **Max we do today:** One revision per job (one “Revised report” per token). You could extend to multiple rounds (e.g. cap at 2–3 revisions per job).

---

## 3. Report length: what’s the max? Can we do 1–20 pages?

- **Default (before optimization):** We used `max_tokens: 2048` for all Kimi report calls. That’s ~1,500 words → about **3–4 pages** single-spaced. So we could not do 1–20 pages in one call.
- **After optimization:** We use a **configurable report max_tokens** (e.g. **8192**). Many providers (Groq, NVIDIA, Moonshot) allow 8192 or more for output.  
  - 8192 tokens ≈ **~6,000 words** ≈ **~12–15 pages** single-spaced. So we **can** generate up to ~15–20 page reports in one API call when the user asks for a long report and we set a higher `max_tokens`.
- **How we maximize/optimize API logic:**
  1. **Single call:** Use `REPORT_MAX_TOKENS` (e.g. 8192) for the main report and for revise-report. That’s the simplest way to get longer reports (up to ~15–20 pages) without chunking.
  2. **Prompt:** In the report prompt we can add: “If the request implies a long or detailed report (e.g. ‘full report’, ‘10+ pages’), produce a comprehensive report using the full length available.”
  3. **Chunked (optional):** For even longer or very structured reports (e.g. 20+ pages), we could generate by sections (outline first, then “Section 2: …” with multiple calls and concatenate). Not implemented by default; single-call 8192 is the main optimization.

---

## 4. Quick reference

| Question | Answer |
|----------|--------|
| What has the AI learned from all the jobs? | Run outcomes and human-front deliveries → **agency-learnings.md**; optionally one-line when user feedback is used for a revision. |
| Why can users add feedback? | To **review the job** and get a **revised report** that implements their feedback. |
| Do we implement feedback and reproduce result? | **Yes.** Revised report is generated by Kimi from original + feedback, stored and shown on the report page. |
| What’s the max we do? | One revision per job (revised report). Report length: **up to ~15–20 pages** with 8192-token output. |
| Can we generate a 1–20 page report with our API? | **Yes.** We set report/revise calls to use **REPORT_MAX_TOKENS** (e.g. 8192) so one call can produce up to ~15–20 pages. |
| How do we maximize/optimize the API for that? | Use higher `max_tokens` (8192) for report and revise; optionally add “long report” wording in the prompt when the request implies length. |

---

## 5. Files involved

| File | Role |
|------|------|
| `admin/server.js` | `generateJobReportWithKimi()`, `/api/revise-report` use `REPORT_MAX_TOKENS` (env: `REPORT_MAX_TOKENS`, default 8192); append to agency-learnings when a revised report is saved. |
| `workspace/cron-results/agency-learnings.md` | Main learning log; brain and run cycle append here. |
| `workspace/job-feedback.json` | User feedback by tracking token. |
| `workspace/revised-reports.json` | Revised report by tracking token (after “Revise report from feedback”). |
